{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pic_folder_name\n",
    "img_cols=32\n",
    "img_rows=32\n",
    "img_channel=1\n",
    "classification_num=10\n",
    "\n",
    "\n",
    "pic_total_num=0\n",
    "total_pic_folder=\"D:/code/train_picture/\"\n",
    "#files=os.listdir(pic_folder)\n",
    "########################################\n",
    "pic_folder_list=[]\n",
    "for num in range(10):\n",
    "    signal_pic_folder=total_pic_folder+str(num)+'/'\n",
    "    files=os.listdir(signal_pic_folder)\n",
    "    for file in files:\n",
    "        if files.index(file)<1200:\n",
    "            pic_total_num=pic_total_num+1\n",
    "            pic_folder_buffer=signal_pic_folder+file\n",
    "            pic_folder_list.append(pic_folder_buffer)\n",
    "np.random.shuffle(pic_folder_list)    \n",
    "np.random.shuffle(pic_folder_list)\n",
    "train_data=np.zeros([pic_total_num,img_rows*img_cols],np.uint8)\n",
    "train_label=np.zeros([pic_total_num,classification_num])\n",
    "print(\"create pic list ok\")\n",
    "#read picture \n",
    "pic_read_num=0\n",
    "\n",
    "for picture_name in pic_folder_list:\n",
    "    img_src=cv2.imread(picture_name)\n",
    "    img_src=cv2.cvtColor(img_src,cv2.COLOR_BGR2GRAY)\n",
    "    img_resize=cv2.resize(img_src,(img_cols,img_rows))\n",
    "#     cv2.namedWindow('1', cv2.WINDOW_NORMAL)\n",
    "#     cv2.imshow(\"1\",img_resize)\n",
    "#     cv2.waitKey(1)\n",
    "    img_reshape=img_resize.reshape(1,img_cols*img_rows)\n",
    "    train_data[pic_read_num]=img_reshape.astype(np.uint8)\n",
    "    train_label[pic_read_num][int(pic_folder_list[pic_read_num][len(total_pic_folder)])]=1  \n",
    "    pic_read_num=pic_read_num+1\n",
    "\n",
    "np.save(\"train_data.npy\",train_data.astype(np.float32))\n",
    "np.save(\"train_label.npy\",train_label)\n",
    "print('create binary ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(pic_folder_list[2][len(total_pic_folder)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data=np.load(\"train_data.npy\")\n",
    "label_data=np.load(\"train_label.npy\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"1\",np.reshape(train_data[0],(img_rows,img_cols)))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-42-29429e08335c>:73: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "iter: 0 acc  0.6053333\n",
      "iter: 1 acc  0.656\n",
      "iter: 2 acc  0.6625\n",
      "iter: 3 acc  0.7273333\n",
      "iter: 4 acc  0.8096667\n",
      "iter: 5 acc  0.8696667\n",
      "iter: 6 acc  0.91175\n",
      "iter: 7 acc  0.93075\n",
      "iter: 8 acc  0.93233335\n",
      "iter: 9 acc  0.93233335\n",
      "iter: 10 acc  0.931\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#train data \n",
    "\n",
    "img_cols=32\n",
    "img_rows=32\n",
    "img_channel=1\n",
    "classification_num=10\n",
    "batch_size=32\n",
    "img_channel=1\n",
    "acc_max=0\n",
    "conv_kernel_1=32\n",
    "conv_kernel_2=64\n",
    "conv_kernel_3=128\n",
    "kernel_size_1=3\n",
    "kernel_size_2=3\n",
    "kernel_size_3=3\n",
    "keep_prob_use=1\n",
    "length_of_fc1=(img_rows//4)*(img_cols//4)*conv_kernel_3\n",
    "fc1_num=1024\n",
    "model_path=\"ckpt/num_char.ckpt\"\n",
    "num_batch=label_data.shape[0]//batch_size\n",
    "#\n",
    "x=tf.placeholder(tf.float32,[None,img_rows*img_cols])\n",
    "#x=tf.placeholder(tf.float32,[img_rows*batch_size,img_cols,img_channel])\n",
    "y=tf.placeholder(tf.float32,[None,classification_num])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "#w=tf.Variable(tf.zeros([img_size,img_size,3,100]))\n",
    "#b=tf.Variable(tf.zeros([100]))\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "def biase_variable(shape):\n",
    "    init=tf.constant(0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def conv_2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "#输入层\n",
    "x_image=tf.reshape(x,[-1,img_rows,img_cols,img_channel])\n",
    "#\n",
    "w_conv1=weight_variable([kernel_size_1,kernel_size_1,img_channel,conv_kernel_1])  # 32 kernels extract feature from 3 plane\n",
    "b_conv1=biase_variable([conv_kernel_1])\n",
    "h_conv1=tf.nn.relu(conv_2d(x_image,w_conv1)+b_conv1)\n",
    "h_pool1=max_pool(h_conv1)\n",
    "\n",
    "\n",
    "w_conv2=weight_variable([kernel_size_2,kernel_size_2,conv_kernel_1,conv_kernel_2])\n",
    "b_conv2=biase_variable([conv_kernel_2])\n",
    "h_conv2=tf.nn.relu(conv_2d(h_pool1,w_conv2)+b_conv2)\n",
    "h_pool2=max_pool(h_conv2)\n",
    "\n",
    "w_conv3=weight_variable([kernel_size_3,kernel_size_3,conv_kernel_2,conv_kernel_3])\n",
    "b_conv3=biase_variable([conv_kernel_3])\n",
    "h_conv3=tf.nn.relu(conv_2d(h_pool2,w_conv3)+b_conv3)\n",
    "\n",
    "\n",
    "w_fc1_buff=weight_variable([length_of_fc1,fc1_num])\n",
    "b_fc1=biase_variable([fc1_num])\n",
    "w_fc1 = tf.nn.dropout(w_fc1_buff, keep_prob) \n",
    "\n",
    "h_pool2_flat=tf.reshape(h_conv3,[-1,length_of_fc1])\n",
    "h_fc1=tf.nn.leaky_relu(tf.matmul(h_pool2_flat,w_fc1)+b_fc1)\n",
    "\n",
    "w_fc2_buff=weight_variable([fc1_num,10])\n",
    "b_fc2=biase_variable([10])\n",
    "w_fc2 = tf.nn.dropout(w_fc2_buff, keep_prob) \n",
    "##\n",
    "saver=tf.train.Saver()\n",
    "prediction=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)\n",
    "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "#tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "train_step=tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
    "#train_step=tf.train.AdadeltaOptimizer(0.005).minimize(cross_entropy)\n",
    "#结果存放在布尔变量中\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "##\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver=tf.train.Saver()\n",
    "    for epoth in range(11):\n",
    "        for batch in range(num_batch):\n",
    "            if batch<(num_batch-1):\n",
    "                batch_x=train_data[(batch*batch_size):((batch+1)*batch_size)]\n",
    "                batch_y=label_data[(batch*batch_size):((batch+1)*batch_size)]\n",
    "                sess.run(train_step,feed_dict={x:batch_x,y:batch_y,keep_prob:keep_prob_use})\n",
    "        acc=sess.run(accuracy,feed_dict={x:train_data,y:label_data,keep_prob:keep_prob_use})\n",
    "        #print(sess.run(prediction,feed_dict={x:x_test,keep_prob:keep_prob_use}))\n",
    "        #acc_data=sess.run(accuracy,feed_dict={x:train_data[0:10000],y:train_label[0:10000],keep_prob:keep_prob_use})\n",
    "        print(\"iter: \"+str(epoth)+\" acc  \"+str(acc))  \n",
    "        if acc>acc_max:\n",
    "            acc_max=acc\n",
    "            saver_path=saver.save(sess,model_path)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/num_char.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output  [[0.09233967 0.10699685 0.10119355 0.10123737 0.09993278 0.09376515\n",
      "  0.09964849 0.1014003  0.10063512 0.10285071]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_cols=32\n",
    "img_rows=32\n",
    "img_channel=1\n",
    "classification_num=10\n",
    "img_channel=1\n",
    "conv_kernel_1=32\n",
    "conv_kernel_2=64\n",
    "conv_kernel_3=128\n",
    "kernel_size_1=3\n",
    "kernel_size_2=3\n",
    "kernel_size_3=3\n",
    "keep_prob_use=1\n",
    "model_path=\"ckpt/num_char.ckpt\"\n",
    "################################\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "##################################\n",
    "length_of_fc1=(img_rows//4)*(img_cols//4)*conv_kernel_3\n",
    "fc1_num=1024\n",
    "x=tf.placeholder(tf.float32,[None,img_rows*img_cols])\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "def weight_variable(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "def biase_variable(shape):\n",
    "    init=tf.constant(0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def conv_2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "#输入层\n",
    "x_image=tf.reshape(x,[-1,img_rows,img_cols,img_channel])\n",
    "#\n",
    "w_conv1=weight_variable([kernel_size_1,kernel_size_1,img_channel,conv_kernel_1])  # 32 kernels extract feature from 3 plane\n",
    "b_conv1=biase_variable([conv_kernel_1])\n",
    "h_conv1=tf.nn.relu(conv_2d(x_image,w_conv1)+b_conv1)\n",
    "h_pool1=max_pool(h_conv1)\n",
    "\n",
    "\n",
    "w_conv2=weight_variable([kernel_size_2,kernel_size_2,conv_kernel_1,conv_kernel_2])\n",
    "b_conv2=biase_variable([conv_kernel_2])\n",
    "h_conv2=tf.nn.relu(conv_2d(h_pool1,w_conv2)+b_conv2)\n",
    "h_pool2=max_pool(h_conv2)\n",
    "\n",
    "w_conv3=weight_variable([kernel_size_3,kernel_size_3,conv_kernel_2,conv_kernel_3])\n",
    "b_conv3=biase_variable([conv_kernel_3])\n",
    "h_conv3=tf.nn.relu(conv_2d(h_pool2,w_conv3)+b_conv3)\n",
    "\n",
    "\n",
    "w_fc1_buff=weight_variable([length_of_fc1,fc1_num])\n",
    "b_fc1=biase_variable([fc1_num])\n",
    "w_fc1 = tf.nn.dropout(w_fc1_buff, keep_prob) \n",
    "\n",
    "h_pool2_flat=tf.reshape(h_conv3,[-1,length_of_fc1])\n",
    "h_fc1=tf.nn.leaky_relu(tf.matmul(h_pool2_flat,w_fc1)+b_fc1)\n",
    "\n",
    "w_fc2_buff=weight_variable([fc1_num,10])\n",
    "b_fc2=biase_variable([10])\n",
    "w_fc2 = tf.nn.dropout(w_fc2_buff, keep_prob) \n",
    "prediction=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver=tf.train.Saver()\n",
    "    save_path=saver.restore(sess,model_path)\n",
    "    acc=sess.run(prediction,feed_dict={x:test_pic,keep_prob:keep_prob_use})\n",
    "    #category=sess.run(tf.argmax(acc,1))\n",
    "    print(\"output  \"+str(acc))\n",
    "    #print(\"this picture is \"+str(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/num_char.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ckpt/num_char.ckpt\n",
      "[[2.8708895e-24 1.0000000e+00 1.3344294e-26 3.8415554e-25 5.4807893e-28\n",
      "  2.1999739e-28 2.8173726e-30 2.6242141e-25 1.7932094e-23 4.7340043e-30]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    save_path=saver.restore(sess,model_path)\n",
    "    \n",
    "    acc=sess.run(prediction,feed_dict={x:test_pic,keep_prob:keep_prob_use})\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63, 65, 62, ..., 40, 42, 44]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.load(\"train_data.npy\")\n",
    "label_data=np.load(\"train_label.npy\") \n",
    "test_pic=train_data[0]\n",
    "\n",
    "test_pic=test_pic.reshape(1,img_cols*img_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.6273206e-24, 1.0000000e+00, 1.5176542e-26, 2.0818073e-26,\n",
       "        8.0995710e-32, 9.6858494e-34, 3.5925025e-33, 6.9558271e-36,\n",
       "        2.3165075e-31, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63., 65., 62., ..., 40., 42., 44.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
