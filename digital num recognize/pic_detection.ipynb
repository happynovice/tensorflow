{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "train_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "img_cols=32\n",
    "img_rows=32\n",
    "img_channel=1\n",
    "classification_num=11\n",
    "folder_pic_num=1200\n",
    "pic_num=0\n",
    "pic_total_num=0\n",
    "pic_num_buffer=0\n",
    "\n",
    "label_data=np.array((1,classification_num))\n",
    "\n",
    "total_pic_folder=\"D:/code/train_picture/\"\n",
    "sort_folder_string=os.listdir(total_pic_folder)\n",
    "def show_image(img):\n",
    "    cv2.namedWindow('1', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"1\",img)\n",
    "    cv2.waitKey(0)\n",
    "def folder_string_change():\n",
    "    signal_pic_folder=total_pic_folder+str(sort_folder_string[num])+'/'\n",
    "    files=os.listdir(signal_pic_folder)\n",
    "    return files\n",
    "def pic_num_count(): \n",
    "    global pic_num\n",
    "    for num in range(classification_num):\n",
    "        signal_pic_folder=total_pic_folder+str(sort_folder_string[num])+'/'\n",
    "        files=os.listdir(signal_pic_folder)\n",
    "        #files=folder_string_change()\n",
    "        for file in files:    \n",
    "            pic_num=pic_num+1\n",
    "\n",
    "def pic_name_store():\n",
    "    global pic_total_num\n",
    "    pic_folder_list=[]\n",
    "    for num in range(classification_num):\n",
    "        signal_pic_folder=total_pic_folder+str(sort_folder_string[num])+'/'\n",
    "        files=os.listdir(signal_pic_folder)\n",
    "        for file in files:\n",
    "            if files.index(file)<1200:\n",
    "                pic_total_num=pic_total_num+1\n",
    "                pic_folder_buffer=signal_pic_folder+file\n",
    "                pic_folder_list.append(pic_folder_buffer)\n",
    "    np.random.shuffle(pic_folder_list)    \n",
    "    np.random.shuffle(pic_folder_list)\n",
    "    return pic_folder_list\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15231\n",
      "create pic list ok\n",
      "create binary ok\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "#create pic_folder_name\n",
    "pic_num_count()\n",
    "print(pic_num)\n",
    "#files=os.listdir(pic_folder)\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "pic_folder_list=pic_name_store()\n",
    "train_data=np.zeros([pic_total_num,img_rows*img_cols],np.uint8)\n",
    "train_label=np.zeros([pic_total_num,classification_num])\n",
    "print(\"create pic list ok\")\n",
    "#read picture \n",
    "pic_read_num=0\n",
    "\n",
    "for picture_name in pic_folder_list:\n",
    "    img_src=cv2.imread(picture_name)\n",
    "    img_src=cv2.cvtColor(img_src,cv2.COLOR_BGR2GRAY)\n",
    "    img_resize=cv2.resize(img_src,(img_cols,img_rows))\n",
    "    #show_image()\n",
    "    img_reshape=img_resize.reshape(1,img_cols*img_rows)\n",
    "    train_data[pic_read_num]=img_reshape.astype(np.uint8)\n",
    "    ######################################################\n",
    "    folder_end=pic_folder_list[pic_read_num].find('/',len(total_pic_folder))\n",
    "    folder_start=pic_folder_list[pic_read_num].find('/',len(total_pic_folder)-1)\n",
    "    folder_label=pic_folder_list[pic_read_num][(folder_start+1):folder_end]\n",
    "    label_num=sort_folder_string.index(folder_label)\n",
    "    train_label[pic_read_num][label_num]=1  \n",
    "    ######################################################\n",
    "    pic_read_num=pic_read_num+1\n",
    "train_data=train_data.astype(np.float32)\n",
    "np.save(\"train_data_back.npy\",train_data.astype(np.float32))\n",
    "np.save(\"train_label_back.npy\",train_label)\n",
    "print('create binary ok')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-3879f8cf22a4>:75: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "iter: 0 acc  0.6814394\n",
      "iter: 1 acc  0.69598484\n",
      "iter: 2 acc  0.7028788\n",
      "iter: 3 acc  0.7442424\n",
      "iter: 4 acc  0.7526515\n",
      "iter: 5 acc  0.81598485\n",
      "iter: 6 acc  0.89484847\n",
      "iter: 7 acc  0.91833335\n",
      "iter: 8 acc  0.98242426\n",
      "iter: 9 acc  0.98704547\n",
      "iter: 10 acc  0.98939395\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#train data \n",
    "\n",
    "train_data=np.load(\"train_data_back.npy\")\n",
    "train_label=np.load(\"train_label_back.npy\") \n",
    "img_cols=32\n",
    "img_rows=32\n",
    "img_channel=1\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "img_channel=1\n",
    "acc_max=0\n",
    "conv_kernel_1=32\n",
    "conv_kernel_2=64\n",
    "conv_kernel_3=128\n",
    "kernel_size_1=3\n",
    "kernel_size_2=3\n",
    "kernel_size_3=3\n",
    "keep_prob_use=1\n",
    "length_of_fc1=(img_rows//4)*(img_cols//4)*conv_kernel_3\n",
    "fc1_num=1024\n",
    "model_path=\"ckpt/num_char_back.ckpt\"\n",
    "num_batch=train_label.shape[0]//batch_size\n",
    "#\n",
    "x=tf.placeholder(tf.float32,[None,img_rows*img_cols])\n",
    "#x=tf.placeholder(tf.float32,[img_rows*batch_size,img_cols,img_channel])\n",
    "y=tf.placeholder(tf.float32,[None,classification_num])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "def biase_variable(shape):\n",
    "    init=tf.constant(0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def conv_2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "#输入层\n",
    "x_image=tf.reshape(x,[-1,img_rows,img_cols,img_channel])\n",
    "#\n",
    "w_conv1=weight_variable([kernel_size_1,kernel_size_1,img_channel,conv_kernel_1])  # 32 kernels extract feature from 3 plane\n",
    "b_conv1=biase_variable([conv_kernel_1])\n",
    "h_conv1=tf.nn.relu(conv_2d(x_image,w_conv1)+b_conv1)\n",
    "h_pool1=max_pool(h_conv1)\n",
    "\n",
    "\n",
    "w_conv2=weight_variable([kernel_size_2,kernel_size_2,conv_kernel_1,conv_kernel_2])\n",
    "b_conv2=biase_variable([conv_kernel_2])\n",
    "h_conv2=tf.nn.relu(conv_2d(h_pool1,w_conv2)+b_conv2)\n",
    "h_pool2=max_pool(h_conv2)\n",
    "\n",
    "w_conv3=weight_variable([kernel_size_3,kernel_size_3,conv_kernel_2,conv_kernel_3])\n",
    "b_conv3=biase_variable([conv_kernel_3])\n",
    "h_conv3=tf.nn.relu(conv_2d(h_pool2,w_conv3)+b_conv3)\n",
    "\n",
    "\n",
    "w_fc1_buff=weight_variable([length_of_fc1,fc1_num])\n",
    "b_fc1=biase_variable([fc1_num])\n",
    "w_fc1 = tf.nn.dropout(w_fc1_buff, keep_prob) \n",
    "\n",
    "h_pool2_flat=tf.reshape(h_conv3,[-1,length_of_fc1])\n",
    "h_fc1=tf.nn.leaky_relu(tf.matmul(h_pool2_flat,w_fc1)+b_fc1)\n",
    "\n",
    "w_fc2_buff=weight_variable([fc1_num,classification_num])\n",
    "b_fc2=biase_variable([classification_num])\n",
    "w_fc2 = tf.nn.dropout(w_fc2_buff, keep_prob) \n",
    "##\n",
    "saver=tf.train.Saver()\n",
    "prediction=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)\n",
    "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "#tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "train_step=tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
    "#train_step=tf.train.AdadeltaOptimizer(0.005).minimize(cross_entropy)\n",
    "#结果存放在布尔变量中\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "##\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver=tf.train.Saver()\n",
    "    for epoth in range(11):\n",
    "        for batch in range(num_batch):\n",
    "            if batch<(num_batch-1):\n",
    "                batch_x=train_data[(batch*batch_size):((batch+1)*batch_size)]\n",
    "                batch_y=train_label[(batch*batch_size):((batch+1)*batch_size)]\n",
    "                sess.run(train_step,feed_dict={x:batch_x,y:batch_y,keep_prob:keep_prob_use})\n",
    "        acc=sess.run(accuracy,feed_dict={x:train_data,y:train_label,keep_prob:keep_prob_use})\n",
    "        #print(sess.run(prediction,feed_dict={x:x_test,keep_prob:keep_prob_use}))\n",
    "        #acc_data=sess.run(accuracy,feed_dict={x:train_data[0:10000],y:train_label[0:10000],keep_prob:keep_prob_use})\n",
    "        print(\"iter: \"+str(epoth)+\" acc  \"+str(acc))  \n",
    "        if acc>acc_max:\n",
    "            acc_max=acc\n",
    "            saver_path=saver.save(sess,model_path)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[11999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"1\",img_reshape.reshape(32,32))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/num_char_back.ckpt\n",
      "this picture shows: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_pic\n",
    "test_pic_src=cv2.imread(\"test_5.png\")\n",
    "test_pic_gray=cv2.cvtColor(test_pic_src,cv2.COLOR_BGR2GRAY)\n",
    "test_pic_resize=cv2.resize(test_pic_gray,(img_cols,img_rows))\n",
    "test_pic_reshape=np.reshape(test_pic_resize,(1,img_cols*img_rows))\n",
    "with tf.Session() as sess:\n",
    "    save_path=saver.restore(sess,model_path)\n",
    "    \n",
    "    acc=sess.run(prediction,feed_dict={x:test_pic_reshape,keep_prob:keep_prob_use})\n",
    "    \n",
    "    print(\"this picture shows: \"+str(np.argmax(acc)))\n",
    "cv2.imshow(\"1\",test_pic_src)\n",
    "cv2.waitKey(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
